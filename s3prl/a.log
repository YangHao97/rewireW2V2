[s3prl.downstream.experts] Warning: can not import s3prl.downstream.a2a-vc-vctk.expert: No module named 'resemblyzer'. Pass.
usage: run_downstream.py [-h] -m {train,evaluate,inference}
                         [-t EVALUATE_SPLIT] [-o OVERRIDE] [--backend BACKEND]
                         [--local_rank LOCAL_RANK] [-e {CKPT_PATH,CKPT_DIR}]
                         [-i CKPT_PATH] [-c CONFIG]
                         [-d {a2o-vc-vcc2020,asr,atis,audio_snips,ctc,diarization,emotion,enhancement_stft,example,fluent_commands,libri_phone,mos_prediction,mosei,phone_1hidden,phone_linear,phone_linear_concat,quesst14_dtw,quesst14_embedding,separation_stft,speaker_linear_frame_libri,speaker_linear_utter_libri,speech_commands,speech_translation,sv_voxceleb1,sws2013,timit_phone,timit_phone_1hidden,timit_phone_linear,timit_phone_linear_concat,voxceleb1,voxceleb1_framelevel,voxceleb2_amsoftmax_segment_eval,voxceleb2_ge2e}]
                         [-v DOWNSTREAM_VARIANT] [--hub {torch,huggingface}]
                         [-u UPSTREAM] [-k {PATH,URL,GOOGLE_DRIVE_ID}]
                         [-g UPSTREAM_MODEL_CONFIG] [-r] [-f]
                         [-s UPSTREAM_FEATURE_SELECTION]
                         [-l UPSTREAM_LAYER_SELECTION]
                         [--upstream_model_name UPSTREAM_MODEL_NAME]
                         [--upstream_revision UPSTREAM_REVISION] [-n EXPNAME]
                         [-p EXPDIR] [-a] [--push_to_hf_hub PUSH_TO_HF_HUB]
                         [--hf_hub_org HF_HUB_ORG] [--seed SEED]
                         [--device DEVICE] [--cache_dir CACHE_DIR] [--verbose]
                         [--disable_cudnn]

optional arguments:
  -h, --help            show this help message and exit
  -m {train,evaluate,inference}, --mode {train,evaluate,inference}
  -t EVALUATE_SPLIT, --evaluate_split EVALUATE_SPLIT
  -o OVERRIDE, --override OVERRIDE
                        Used to override args and config, this is at the
                        highest priority
  --backend BACKEND     The backend for distributed training
  --local_rank LOCAL_RANK
                        The GPU id this process should use while distributed
                        training. None when not launched by
                        torch.distributed.launch
  -e {CKPT_PATH,CKPT_DIR}, --past_exp {CKPT_PATH,CKPT_DIR}
                        Resume training from a checkpoint
  -i CKPT_PATH, --init_ckpt CKPT_PATH
                        Load the checkpoint for evaluation
  -c CONFIG, --config CONFIG
                        The yaml file for configuring the whole experiment
                        except the upstream model
  -d {a2o-vc-vcc2020,asr,atis,audio_snips,ctc,diarization,emotion,enhancement_stft,example,fluent_commands,libri_phone,mos_prediction,mosei,phone_1hidden,phone_linear,phone_linear_concat,quesst14_dtw,quesst14_embedding,separation_stft,speaker_linear_frame_libri,speaker_linear_utter_libri,speech_commands,speech_translation,sv_voxceleb1,sws2013,timit_phone,timit_phone_1hidden,timit_phone_linear,timit_phone_linear_concat,voxceleb1,voxceleb1_framelevel,voxceleb2_amsoftmax_segment_eval,voxceleb2_ge2e}, --downstream {a2o-vc-vcc2020,asr,atis,audio_snips,ctc,diarization,emotion,enhancement_stft,example,fluent_commands,libri_phone,mos_prediction,mosei,phone_1hidden,phone_linear,phone_linear_concat,quesst14_dtw,quesst14_embedding,separation_stft,speaker_linear_frame_libri,speaker_linear_utter_libri,speech_commands,speech_translation,sv_voxceleb1,sws2013,timit_phone,timit_phone_1hidden,timit_phone_linear,timit_phone_linear_concat,voxceleb1,voxceleb1_framelevel,voxceleb2_amsoftmax_segment_eval,voxceleb2_ge2e}
                        Typically downstream dataset need manual preparation.
                        Please check downstream/README.md for details
  -v DOWNSTREAM_VARIANT, --downstream_variant DOWNSTREAM_VARIANT
                        Downstream vairants given the same expert
  --hub {torch,huggingface}
                        The model Hub used to retrieve the upstream model.
  -u UPSTREAM, --upstream UPSTREAM
                        Upstreams with "_local" or "_url" postfix need local
                        ckpt (-k) or config file (-g). Other upstreams
                        download two files on-the-fly and cache them, so just
                        -u is enough and -k/-g are not needed. Please check
                        upstream/README.md for details. Available options in
                        S3PRL: ['apc', 'apc_360hr', 'apc_960hr', 'apc_local',
                        'apc_url', 'audio_albert', 'audio_albert_960hr',
                        'audio_albert_gdriveid', 'audio_albert_local', 'audio_
                        albert_logMelBase_T_share_AdamW_b32_1m_960hr_drop1',
                        'audio_albert_url', 'baseline', 'baseline_local',
                        'byol_a', 'byol_a_1024', 'byol_a_2048', 'byol_a_512',
                        'byol_a_gdriveid', 'byol_a_local', 'byol_a_url',
                        'cpc_local', 'cpc_url', 'customized_upstream',
                        'decoar', 'decoar2', 'decoar2_local', 'decoar2_url',
                        'decoar_layers', 'decoar_layers_local',
                        'decoar_layers_url', 'decoar_local', 'decoar_url',
                        'distilhubert', 'distilhubert_base',
                        'distiller_local', 'distiller_url', 'fbank',
                        'fbank_no_cmvn', 'hubert', 'hubert_base',
                        'hubert_large_ll60k', 'hubert_local', 'hubert_url',
                        'libri_posteriorgram', 'libri_posteriorgram_local',
                        'libri_posteriorgram_url', 'linear', 'mel', 'mfcc',
                        'mockingjay', 'mockingjay_100hr', 'mockingjay_960hr',
                        'mockingjay_gdriveid', 'mockingjay_local',
                        'mockingjay_logMelBase_T_AdamW_b32_1m_960hr',
                        'mockingjay_logMelBase_T_AdamW_b32_1m_960hr_drop1',
                        'mockingjay_logMelBase_T_AdamW_b32_1m_960hr_seq3k',
                        'mockingjay_logMelBase_T_AdamW_b32_200k_100hr', 'mocki
                        ngjay_logMelLinearLarge_T_AdamW_b32_500k_360hr_drop1',
                        'mockingjay_origin', 'mockingjay_url', 'modified_cpc',
                        'mos_apc', 'mos_apc_local', 'mos_apc_url', 'mos_tera',
                        'mos_tera_local', 'mos_tera_url', 'mos_wav2vec2',
                        'mos_wav2vec2_local', 'mos_wav2vec2_url', 'npc',
                        'npc_360hr', 'npc_960hr', 'npc_local', 'npc_url',
                        'spec_augment', 'spec_augment_gdriveid',
                        'spec_augment_local', 'spec_augment_url',
                        'spectrogram', 'stft_mag', 'tera', 'tera_100hr',
                        'tera_960hr',
                        'tera_fbankBase_T_F_AdamW_b32_200k_100hr',
                        'tera_gdriveid', 'tera_local',
                        'tera_logMelBase_T_F_AdamW_b32_1m_960hr',
                        'tera_logMelBase_T_F_AdamW_b32_1m_960hr_drop1',
                        'tera_logMelBase_T_F_AdamW_b32_1m_960hr_seq3k',
                        'tera_logMelBase_T_F_AdamW_b32_200k_100hr',
                        'tera_logMelBase_T_F_M_AdamW_b32_1m_960hr_drop1',
                        'tera_logMelBase_T_F_M_AdamW_b32_200k_100hr',
                        'tera_url', 'timit_posteriorgram',
                        'timit_posteriorgram_local',
                        'timit_posteriorgram_url', 'unispeech_sat',
                        'unispeech_sat_base', 'unispeech_sat_base_plus',
                        'unispeech_sat_large', 'unispeech_sat_local',
                        'unispeech_sat_url', 'vq_apc', 'vq_apc_360hr',
                        'vq_apc_960hr', 'vq_apc_url', 'vq_wav2vec',
                        'vq_wav2vec_gumbel', 'vq_wav2vec_kmeans',
                        'vq_wav2vec_kmeans_roberta', 'vq_wav2vec_local',
                        'vq_wav2vec_url', 'wav2vec', 'wav2vec2',
                        'wav2vec2_base_960', 'wav2vec2_hug',
                        'wav2vec2_hug_base_960', 'wav2vec2_hug_large_ll60k',
                        'wav2vec2_large_960', 'wav2vec2_large_ll60k',
                        'wav2vec2_local', 'wav2vec2_url', 'wav2vec2_xlsr',
                        'wav2vec_large', 'wav2vec_local', 'wav2vec_url',
                        'wavlm', 'wavlm_base', 'wavlm_base_plus',
                        'wavlm_large', 'wavlm_local', 'wavlm_url'].
  -k {PATH,URL,GOOGLE_DRIVE_ID}, --upstream_ckpt {PATH,URL,GOOGLE_DRIVE_ID}
                        Only set when the specified upstream need it
  -g UPSTREAM_MODEL_CONFIG, --upstream_model_config UPSTREAM_MODEL_CONFIG
                        The config file for constructing the pretrained model
  -r, --upstream_refresh
                        Re-download cached ckpts for on-the-fly upstream
                        variants
  -f, --upstream_trainable
                        Fine-tune, set upstream.train(). Default is
                        upstream.eval()
  -s UPSTREAM_FEATURE_SELECTION, --upstream_feature_selection UPSTREAM_FEATURE_SELECTION
                        Specify the layer to be extracted as the
                        representation
  -l UPSTREAM_LAYER_SELECTION, --upstream_layer_selection UPSTREAM_LAYER_SELECTION
                        Select a specific layer for the features selected by
                        -s
  --upstream_model_name UPSTREAM_MODEL_NAME
                        The name of the model file in the HuggingFace Hub
                        repo.
  --upstream_revision UPSTREAM_REVISION
                        The commit hash of the specified HuggingFace
                        Repository
  -n EXPNAME, --expname EXPNAME
                        Save experiment at result/downstream/expname
  -p EXPDIR, --expdir EXPDIR
                        Save experiment at expdir
  -a, --auto_resume     Auto-resume if the expdir contains checkpoints
  --push_to_hf_hub PUSH_TO_HF_HUB
                        Push all files in experiment directory to the Hugging
                        Face Hub. To use this feature you must set HF_USERNAME
                        and HF_PASSWORD as environment variables in your shell
  --hf_hub_org HF_HUB_ORG
                        The Hugging Face Hub organisation to push fine-tuned
                        models to
  --seed SEED
  --device DEVICE       model.to(device)
  --cache_dir CACHE_DIR
                        The cache directory for pretrained model downloading
  --verbose             Print model infomation
  --disable_cudnn       Disable CUDNN
